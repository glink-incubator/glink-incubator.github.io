[{"id":0,"href":"/applications/spatial_filter/","title":"Spatial Filter","parent":"Applications","content":"    SpatialDataStream API Spatial SQL API     Spatial Filter用于进行时间无关的无界空几间数据处理. 一般用于对无界空间数据进行基于空间关系的过滤操作, 比如对轨迹数据流进行过滤, 以保留落在某个指定行政区内的轨迹数据. 它包含三个要素:\n 无界空间数据: 被过滤的主体. 查询几何: 用来对空间数据流进行过滤的几何图形. 拓扑关系: 指示流几何与查询几何在空间上处于何种拓扑关系时, 该记录将被保留.  Glink在SpatialDataStream API中对Spatial Filter提供了完善的支持, 包括支持所有符合OGC标准的几何要素对象以及拓扑关系, 以及支持多个查询几何的场景, 并对此进行了优化. Spatial SQL API中同样提供了对Spatial Filter的支持, 不过存在一定限制.\nSpatialDataStream API 以下是一个使用SpatialDataStream API进行Spatial Filter的案例. 该案例对一个无界空间点数据进行过滤, 所有包含在多边形 POLYGON ((10 10, 10 20, 20 20, 20 10, 10 10))内的点将被保留, 其他点将被过滤. 我们在Glink的源代码中提供了一个可运行的Spatial Filter案例, 具体可参见SpatialFilterExample.\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Geometry queryGeometry = ... // e.g \u0026#34;POLYGON ((10 10, 10 20, 20 20, 20 10, 10 10))\u0026#34;  SpatialDataStream\u0026lt;Point\u0026gt; pointDataStream = new SpatialDataStream\u0026lt;\u0026gt;( env, \u0026#34;localhost\u0026#34;, 9999, new SimplePointFlatMapper()); DataStream\u0026lt;Point\u0026gt; resultStream = SpatialFilter.filter( pointDataStream, queryGeometry, TopologyType.CONTAINS); Glink提供了完整的拓扑关系模型, 兼容OGC标准, 并在此基础上进行了扩展, Spatial Filter所支持的拓扑关系类型如下. 在 SpatialDataStream API 中, 可通过TopologyType枚举类进行指定.\n   拓扑关系 枚举类型 是否 OGC 标准     包含 CONTAINS 是   交叉 CROSSES 是   相离 DISJOINT 是   相等 EQUAL 是   相交 INTERSECTS 是   覆盖 OVERLAPS 是   接触 TOUCH 是   包含于 WITHIN 是   距离包含于 WITHIN_DISTANCE 否   缓冲 BUFFER 否    需要注意的是, 这里的拓扑关系是流几何对于查询几何而言的.\nSpatial SQL API Glink SQL提供了多个空间函数, 可用于进行Spatial Filter. 函数列表如下.\n   Spatial SQL函数 拓扑关系     ST_Contains CONTAINS   ST_Covers COVERS   ST_Crosses CROSSES   ST_Disjoint DISJOINT   ST_Equals EQUALS   ST_Intersects INTERSECTS   ST_Overlaps OVERLAPS   ST_Touches TOUCHES   ST_Within WITHIN    使用Spatial SQL函数可以实现简单的Spatial Filter, 如下案例实现了与上述SpatialDataStream API中案例相同的语义.\nCREATE TABLE point_table ( id STRING, dtg TIMESTAMP(0), lng DOUBLE NOT NULL, lat DOUBLE NOT NULL, ) WITH ( ... ) SELECT * FROM point_table WHERE ST_Contains( ST_PolygonFromText(\u0026#39;POLYGON ((10 10, 10 20, 20 20, 20 10, 10 10))\u0026#39;), ST_Point(lng, lat) ); 这里需要注意的是, 尽管在Spatial SQL中我们可以通过串联多个空间关系判断函数实现多个查询几何的Spatial Filter, 但是这毫无疑问是低效的, 我们需要遍历所有的查询几何并进行判断. 相比之下, SpatialDataStream API在多个查询几何的情况下进行了优化, 它首先会为所有的查询几何建立R-tree, 这样在流几何与与查询几何进行空间关系判断时可借助R-tree索引, 从而避免了每次判断都遍历所有查询几何, 这在查询几何数量较多的情况下尤为明显.\n在Spatial SQL API中如果要实现包含大量查询几何的Spatial Filter场景, 可借助lookup join功能, 把查询几何存储在外部存储引擎中, 关于如何在Glink中进行spatial lookup join可参考Spatial Dimension Join.\n "},{"id":1,"href":"/applications/spatial_window_knn/","title":"Spatial Window KNN","parent":"Applications","content":"  Spatial Window KNN用于实现窗口化的KNN查询, 即根据输入的查询点, 实时输出每个时间窗口内距离(目前仅支持欧式距离)查询点最近的K个流几何. 它包含以下几个要素:\n 无界空间数据: 被查询的主体, 将根据指定的窗口长度进行划分. 查询点: 用来进行查询的空间点. K值: 当前窗口中与查询点距离最近的K个流几何将被保留. 窗口: 定义窗口的划分方式, 每个窗口内的数据将分别进行KNN查询.  Glink目前仅在SpatialDataStream API中提供了Spatial Window KNN的支持, 以下是一个案例. 该案例将对无界空间点数据进行滑动窗口划分, 窗口长度为5s, 并对每个窗口内的数据执行KNN查询, 距离空间点POINT (100.5, 30.5)最近的3个流几何将被输出. 我们在Glink的源代码中提供了一个可直接运行的案例, 具体可参见SpatialWindowKNNExample.\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Point queryPoint = SpatialDataStream.geometryFactory.createPoint(new Coordinate(100.5, 30.5)); SpatialDataStream\u0026lt;Point\u0026gt; pointDataStream = new SpatialDataStream\u0026lt;\u0026gt;( env, pointTextPath, new SimpleSTPointFlatMapper()); pointDataStream.assignTimestampsAndWatermarks(WatermarkStrategy .\u0026lt;Point\u0026gt;forMonotonousTimestamps() .withTimestampAssigner((point, time) -\u0026gt; { Tuple2\u0026lt;String, Long\u0026gt; userData = (Tuple2\u0026lt;String, Long\u0026gt;) point.getUserData(); return userData.f1; })); DataStream\u0026lt;Point\u0026gt; knnStream = SpatialWindowKNN.knn( pointDataStream, queryPoint, 3, Double.MAX_VALUE, TumblingEventTimeWindows.of(Time.seconds(5))); "},{"id":2,"href":"/applications/spatial_join/","title":"Spatial Join","parent":"Applications","content":"    Spatial Dimension Join  SpatialDataStream API Spatial SQL API   Spatial Window Join Spatial Interval Join     Glink提供了三种类型的Spatial Join, 分别是用于流-维Join的Spatial Dimension Join, 以及用于双流Join的Spatial Window Join和Spatial Interval Join. 下面将分别介绍.\nSpatial Dimension Join Spatial Dimension Join用于流-维Join, 可以实现无界空间数据中流几何与空间维度表中维度几何的空间连接. 比如现在有一个轨迹数据流, 以及相应的区县级行政区划(每个区/县的地理范围都对应一个地理多边形), Spatial Dimension Join可将二者进行空间连接, 每个轨迹点与空间上包含其的行政区连接.\nGlink在SpatialDataStream API以及Spatial SQL API中均提供了对Spatial Dimension Join的支持, 不过目前实现方式有所区别. 在SpatialDataStream中, Glink将空间维度表以BraodcastDataStream的形式表示, 在内存中为空间维度表建立了R-tree索引, 吞吐量较高; 在Spatial SQL中, 借助lookup join语法实现, 空间维度表必须存储在外部引擎中, 目前仅支持GeoMesa HBase Datastore, 由于每个流几何的连接都需要访问外部存储, 其吞吐量相对较低, 仍存在进一步优化的空间.\nSpatialDataStream API 以下是一个使用SpatialDataStream API进行Spatial Dimension Join的案例. 该案例对一个无界空间点数据pointDataStream与一个空间维度表spatialDataStream进行连接, 当维度几何在空间上包含流几何时(由TopologyType.N_CONTAINS指定), 即将二者进行连接操作.\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SpatialDataStream\u0026lt;Point\u0026gt; pointDataStream = new SpatialDataStream\u0026lt;\u0026gt;( env, \u0026#34;localhost\u0026#34;, 8888, new SimplePointFlatMapper()); BroadcastSpatialDataStream\u0026lt;Geometry\u0026gt; spatialDataStream = new BroadcastSpatialDataStream\u0026lt;\u0026gt;( env, \u0026#34;localhost\u0026#34;, 9999, new WKTGeometryBroadcastMapper()); SpatialDimensionJoin.join( pointDataStream, spatialDataStream, TopologyType.N_CONTAINS, Tuple2::new, new TypeHint\u0026lt;Tuple2\u0026lt;Point, Geometry\u0026gt;\u0026gt;() { }); Spatial SQL API 在Spatial SQL中, Glink借助Flink的lookup join语法实现了Spatial Dimension Join. 由于lookup join必须借助外部存储, 目前Glink仅支持GeoMesa作为外部存储, 具体可见GeoMesa SQL Connector.\n关于使用Spatial SQL实现Spatial Dimension Join的案例可参考DDL中指定空间连接谓词.\nSpatial Window Join Spatial Window Join支持两个无界空间数据的连接, 在时间维度上与Flink的window join有相同的语义. 以下案例对两个无界空间点数据进行空间连接, 在每个5s的滑动窗口内, 只要两个无界空间数据中的流几何距离在1KM以内即会进行连接.\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SpatialDataStream\u0026lt;Point\u0026gt; pointSpatialDataStream1 = new SpatialDataStream\u0026lt;\u0026gt;(env, \u0026#34;localhost\u0026#34;, 9000, new SimpleSTPointFlatMapper()) .assignTimestampsAndWatermarks(WatermarkStrategy .\u0026lt;Point\u0026gt;forBoundedOutOfOrderness(Duration.ZERO) .withTimestampAssigner((event, time) -\u0026gt; ((Tuple2\u0026lt;String, Long\u0026gt;) event.getUserData()).f1)); SpatialDataStream\u0026lt;Point\u0026gt; pointSpatialDataStream2 = new SpatialDataStream\u0026lt;\u0026gt;(env, \u0026#34;localhost\u0026#34;, 9001, new SimpleSTPointFlatMapper()) .assignTimestampsAndWatermarks(WatermarkStrategy .\u0026lt;Point\u0026gt;forBoundedOutOfOrderness(Duration.ZERO) .withTimestampAssigner((event, time) -\u0026gt; ((Tuple2\u0026lt;String, Long\u0026gt;) event.getUserData()).f1)); DataStream\u0026lt;Tuple2\u0026lt;Point, Point\u0026gt;\u0026gt; windowJoinStream = SpatialWindowJoin.join( pointSpatialDataStream1, pointSpatialDataStream2, TopologyType.WITHIN_DISTANCE.distance(1), TumblingEventTimeWindows.of(Time.seconds(5))); Spatial Interval Join Spatial Interval Join同样支持两个无界空间数据的连接, 在时间维度上与Flink的interval join有相同的语义. 以下案例对两个无界空间点数据进行连接, 对一个无界空间数据中的任意一个流几何, 连接另一个无界空间点数据中在前后5s时间窗口内的数据.\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SpatialDataStream\u0026lt;Point\u0026gt; pointSpatialDataStream1 = new SpatialDataStream\u0026lt;\u0026gt;(env, \u0026#34;localhost\u0026#34;, 8888, new SimpleSTPointFlatMapper()) .assignBoundedOutOfOrdernessWatermarks(Duration.ZERO, 1); SpatialDataStream\u0026lt;Point\u0026gt; pointSpatialDataStream2 = new SpatialDataStream\u0026lt;\u0026gt;(env, \u0026#34;localhost\u0026#34;, 9999, new SimpleSTPointFlatMapper()) .assignBoundedOutOfOrdernessWatermarks(Duration.ZERO, 1); DataStream\u0026lt;Tuple2\u0026lt;Point, Point\u0026gt;\u0026gt; joinStream = SpatialIntervalJoin.join( pointSpatialDataStream1, pointSpatialDataStream2, TopologyType.WITHIN_DISTANCE.distance(1), Time.seconds(-5), Time.seconds(5)); "},{"id":3,"href":"/applications/spatial_window_dbscan/","title":"Spatial Window DBSCAN","parent":"Applications","content":"  Spatial Window DBSCAN用于实现窗口化的DBSCAN聚类, 即对每个窗口内的数据执行DBSCAN聚类并即时输出聚类结果. 它包含以下几个要素:\n 无界点数据: 被查询的主体, 将根据指定的窗口长度进行划分, 对于DBSCAN聚类只支持点对象. DBSCAN参数: 包括距离阈值ε和最小点数minPts. 窗口: 定义窗口的划分方式, 每个窗口内的数据将分别进行DBSCAN聚类.  Glink目前仅在SpatialDataStream API中提供了Spatial Window DBSCAN的支持, 以下是一个案例. 该案例将对无界空间点数据进行滑动窗口划分, 窗口长度为5min, 并对每个窗口内的数据执行DBSCAN聚类, 距离阈值为0.15KM, 最小点数为3. 我们在Glink的源代码中提供了一个可直接运行的案例, 具体可参见WindowDBSCANExample.\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SpatialDataStream\u0026lt;Point2\u0026gt; pointDataStream = new SpatialDataStream\u0026lt;\u0026gt;( env, path, new SimpleSTPoint2FlatMapper()) .assignTimestampsAndWatermarks(WatermarkStrategy .\u0026lt;Point2\u0026gt;forMonotonousTimestamps() .withTimestampAssigner((p, time) -\u0026gt; p.getTimestamp())); DataStream\u0026lt;Tuple2\u0026lt;Integer, List\u0026lt;Point2\u0026gt;\u0026gt;\u0026gt; dbscanStream = WindowDBSCAN.dbscan( pointDataStream, 0.15, 3, TumblingEventTimeWindows.of(Time.minutes(5))); "},{"id":4,"href":"/connectors/overview/","title":"Overview","parent":"Connectors","content":"  Glink需要与外部存储引擎交互, 以从中摄取数据进行计算或输出计算结果. 在Glink中, 外部存储引擎通常有以下几种用途:\n 在ETL场景中, 作为数据输入(source), 或计算结果的输出(sink). source端一般是支持流式消费的消息队列或文件, sink端既可以是消息队列和文件(用于下游流计算), 也可以是数据库(用于查询或分析). 借助外部存储引擎的时空索引能力, 进行空间lookup join (spatial lookup join).  然而, 由于Flink目前并不支持空间数据类型, 且无法支持可注册的自定义类型, 因此Flink现有的Connector对空间数据类型的支持是有限制的.\n 对于像Kafka这种无schema的存储引擎, 可以用Glink提供的Spatial SQL将空间数据转换为WKT/WKB进行存储. 对于关系型数据库, 尽管MySQL, PostreSQL都支持空间数据存储, 但是直接用Flink JDBC Connector是无法写入空间数据类型的, 因为这些数据库需要严格的类型定义, 而目前在Flink中无法在CREATE TABLE DDL中定义空间数据类型. 当然, 我们可以以WKT/WKB的形式将空间数据存储到关系型数据库中, 遗憾的是这样我们只能简单的存储数据, 无法使用存储引擎的空间索引能力.  不过好在Flink一般在大数据量的场景下使用, 在大数据场景下目前最为流行的时空数据存储引擎是GeoMesa, 底层支持各类分布式存储引擎. 为此Glink通过扩展Flink的Connector框架, 提供了GeoMesa Connector, 它支持完备的空间数据类型和空间关系.\n"},{"id":5,"href":"/connectors/geomesa/","title":"GeoMesa SQL Connector","parent":"Connectors","content":"    Dependencies DDL  DDL中定义空间数据类型 DDL中指定空间连接谓词   How to use GeoMesa table Connector Options  GeoMesa HBase Data Store   Data Type Mapping  Basic Data Types Spatial Data Types       Glink对Flink的SQL Connector进行了扩展, 实现了GeoMesa SQL Connector. 这里介绍了如何设置和使用GeoMesa SQL Connector. 我们在GeoMesa 3.1+版本上进行了测试.\nDependencies 为了使用GeoMesa SQL Connector, 需要添加如下依赖.\n   Glink dependency     glink-connector-geomesa-x.x.x.jar   glink-sql-x.x.x.jar    最简单的方法是将$GLINK_HOME/lib目录中的上述两个Jar包复制到$FLINK_HOME/lib目录下.\nDDL 使用GeoMesa SQL Connector时, 在DDL中有几点需要特别注意, 这里进行详细说明.\nDDL中定义空间数据类型 由于Flink当前无法支持可注册的自定义类型, 因此我们无法在DDL中直接定义空间数据类型. 在GeoMesa SQL Connector中可以WKT/WKB形式表示空间数据类型, 并在WITH参数中用geomesa.spatial.fields指明, 其格式为: \u0026lt;field name\u0026gt;:\u0026lt;field type\u0026gt;, 多个字段由\u0026rdquo;,\u0026ldquo;分隔. 其中\u0026lt;field type\u0026gt;支持Spatial Data Types中的所有GeoMesa Type.\n如下DDL定义了GeoMesa中的一个表, 用于存储T-Drive数据, 其中point2字段为WKT格式的STRING类型, 并且在WITH参数中将其指定为Point空间类型. 这样, 如果将其作为source, 那么GeoMesa SQL Connector从GeoMesa取出数据时会将point2字段的数据类型从Point转化为WKT格式的STRING; 如果将其作为sink, 那么GeoMesa SQL Connector将数据写入GeoMesa时会将WKT格式的STRING转化为Point.\nCREATE TABLE GeoMesa_TDrive ( `pid` STRING, `time` TIMESTAMP(0), `point2` STRING, PRIMARY KEY (pid) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;geomesa\u0026#39;, \u0026#39;geomesa.data.store\u0026#39; = \u0026#39;hbase\u0026#39;, \u0026#39;geomesa.schema.name\u0026#39; = \u0026#39;geomesa-test\u0026#39;, \u0026#39;geomesa.spatial.fields\u0026#39; = \u0026#39;point2:Point\u0026#39;, \u0026#39;hbase.catalog\u0026#39; = \u0026#39;test-sql\u0026#39; ); DDL中指定空间连接谓词 目前Flink仅支持等值连接, 因此在Glink中使用GeoMesa进行lookup join时, 我们必须为每个连接指定其空间关系. 我们通过在WITH参数中增加geomesa.temporal.join.predict选项来实现. geomesa.temporal.join.predict目前支持以下选项:\n R:\u0026lt;distance\u0026gt;:表示维度几何与流几何距离小于distance米即符合空间连接条件; I: 表示流几何与维度几何相交即符合空间连接条件; +C: 表示流几何包含维度几何即符合空间连接条件; -C: 表示维度几何包含流几何即符合空间连接条件.  以下是一个使用GeoMesa SQL Connector进行lookup join的案例, 它读取CSV文件中的点数据, 将其与GeoMesa中的多边形数据进行空间连接, 如果点被包含在某个多边形中, 则将二者进行连接. 注意'geomesa.temporal.join.predict' = 'I'这一行, 它指定了空间连接的类型为包含(对于多边形与点之间的空间关系, 包含和相交是一样的), 这样在DQL的连接条件ON ST_AsText(ST_Point(A.lng, A.lat)) = B.geom中, Glink就会将=理解为两侧的几何类型相交.\nCREATE TABLE csv_point ( id STRING, dtg TIMESTAMP(0), lng DOUBLE, lat DOUBLE, proctime AS PROCTIME()) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/path/to/csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); CREATE TABLE GeoMesa_Area ( id STRING, dtg TIMESTAMP(0), geom STRING, PRIMARY KEY (id) NOT ENFORCED) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;geomesa\u0026#39;, \u0026#39;geomesa.data.store\u0026#39; = \u0026#39;hbase\u0026#39;, \u0026#39;geomesa.schema.name\u0026#39; = \u0026#39;restricted_area\u0026#39;, \u0026#39;geomesa.spatial.fields\u0026#39; = \u0026#39;geom:Polygon\u0026#39;, \u0026#39;geomesa.temporal.join.predict\u0026#39; = \u0026#39;I\u0026#39;, \u0026#39;hbase.zookeepers\u0026#39; = \u0026#39;localhost:2181\u0026#39;, \u0026#39;hbase.catalog\u0026#39; = \u0026#39;restricted_area\u0026#39; ); SELECT A.id AS point_id, A.dtg, ST_AsText(ST_Point(A.lng, A.lat)) AS point, B.id AS area_id FROM csv_point AS A LEFT JOIN GeoMesa_Area FOR SYSTEM_TIME AS OF A.proctime AS B ON ST_AsText(ST_Point(A.lng, A.lat)) = B.geom; How to use GeoMesa table 上述案例中已经使用过GeoMesa表, 这里再用一个完整的例子进一步阐述. 在这个例子中, 我们实现的功能是使用Glink将CSV中的点数据导入到GeoMesa中, 它几乎是一个完整的空间数据ETL案例了.\n首先创建CSV source table.\nCREATE TABLE CSV_TDrive ( `pid` STRING, `time` TIMESTAMP(0), `lng` DOUBLE, `lat` DOUBLE ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/path/to/csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); 然后创建GeoMesa sink table.\nCREATE TABLE Geomesa_TDrive ( `pid` STRING, `time` TIMESTAMP(0), `point2` STRING, PRIMARY KEY (pid) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;geomesa\u0026#39;, \u0026#39;geomesa.data.store\u0026#39; = \u0026#39;hbase\u0026#39;, \u0026#39;geomesa.schema.name\u0026#39; = \u0026#39;geomesa-test\u0026#39;, \u0026#39;geomesa.spatial.fields\u0026#39; = \u0026#39;point2:Point\u0026#39;, \u0026#39;hbase.catalog\u0026#39; = \u0026#39;test-sql\u0026#39; ); 最后执行以下DML语句即可执行ETL作业.\nINSERT INTO Geomesa_TDrive SELECT `pid`, `time`, ST_AsText(ST_Point(lng, lat)) FROM CSV_TDrive; Connector Options GeoMesa SQL Connector支持通过WITH参数的方式对GeoMesa客户端的相关参数进行配置. 其中有些参数是引擎无关的, 有些参数是不同后端存储引擎的可选配置, 具体如下.\nGeoMesa 以下配置是不受GeoMesa后端存储引擎所影响的.\n   Option Required Description     connector required 连接器类型, 对于GeoMesa SQL Connector而言固定为geomesa   geomesa.data.store required GeoMesa Data Store类型, 目前支持hbase   geomesa.schema.name required GeoMesa Schema名称   geomesa.spatial.fields optional 空间类型字段, 当包含空间字段时必须指定, 否则空间类型将无法正确解析, 格式: :, 多个字段间由\u0026rdquo;,\u0026ldquo;分隔   geomesa.temporal.join.predict optional 指定lookup join的空间关系谓词, 符合关系的记录将被join:\nR:表示维表中空间对象与流表中空间对象距离小于distance米;\nI表示流表中空间对象与维表中空间对象相交;\n+C表示流表中空间对象包含维表中空间对象;\n-C表示维表中空间对象包含流表中空间对象.    HBase Data Store 以下参数是GeoMesa以HBase作为后端存储引擎时可选的配置参数. GeoMesa SQL Connector支持GeoMesa HBase Data Store的所有配置参数, 关于各个参数的具体含义, 参见Geomesa文档.\n   Option Required     hbase.catalog optional   hbase.zookeepers optional   hbase.coprocessor.url optional   hbase.config.paths optional   hbase.config.xml optional   hbase.connections.reuse optional   hbase.remote.filtering optional   hbase.security.enabled optional   hbase.coprocessor.threads optional   hbase.ranges.max-per-extended-scan optional   hbase.ranges.max-per-coprocessor-scan optional   hbase.coprocessor.arrow.enable optional   hbase.coprocessor.bin.enable optional   hbase.coprocessor.density.enable optional   hbase.coprocessor.stats.enable optional   hbase.coprocessor.yield.partial.results optional   hbase.coprocessor.scan.parallel optional    注意: 当geomesa.data.store为hbase时必须指定hbase.catalog  Data Type Mapping Flink SQL的数据类型并不与GeoMesa完全兼容. 对于基础数据类型而言GeoMesa SQL Connector进行了最大程度的适配; 对于空间数据类型, 由于Flink目前尚未支持可注册的结构化类型, 因此在Flink SQL中所有空间数据类型均由WKT/WKB格式的SRING/BINARY类型表示, 且必须使用geomesa.spatial.fields这一WITH参数指定具体类型, Geomesa SQL Connector在写入时会使用org.locationtech.jts.io.WKTReader/org.locationtech.jts.io.WKBReader进行转换. 详细的数据类型对应关系如下. Flink SQL数据类型参见Flink文档. GeoMesa数据类型参见GeoMesa文档.\nBasic Data Types    Flink SQL Type GeoMesa Type Java Type Indexable     CHAR / VARCHAR / STRING String java.lang.String Yes   BOOLEAN Boolean java.lang.Boolean Yes   BINARY / VARBINARY / BYTES Bytes byte[] No   TINYINT / SMALLINT / INT Integer java.lang.Integer Yes   BIGINT Long java.lang.Long Yes   FLOAT Float java.lang.Float Yes   DOUBLE Double java.lang.Double Yes   DATE / TIME Date java.util.Date Yes   TIMESTAMP Timestamp java.sql.Timestamp Yes   DECIMAL Not supported     ARRAY Not supported     MAP / MULTISET Not supported     Row Not supported      Spatial Data Types 所有空间数据类型在Glink SQL中均由WKT/WKB格式的STRING/ARRAY类型表示.\n   GeoMesa Type Java Type Indexable     Point org.locationtech.jts.geom.Point Yes   LineString org.locationtech.jts.geom.LineString Yes   Polygon org.locationtech.jts.geom.Polygon Yes   MultiPoint org.locationtech.jts.geom.MultiPoint Yes   MultiLineString org.locationtech.jts.geom.MultiLineString Yes   MultiPolygon org.locationtech.jts.geom.MultiPolygon Yes   GeometryCollection org.locationtech.jts.geom.GeometryCollection Yes   Geometry org.locationtech.jts.geom.Geometry Yes    "},{"id":6,"href":"/glink_sql/glink_sql_functions/","title":"Glink SQL Functions","parent":"Glink SQL","content":"  Glink在Flink SQL框架上扩展了一系列Spatial SQL函数, 具体函数列表和定义可参考GeoMesa SparkSQL Functions.\n"},{"id":7,"href":"/introduction/","title":"Introduction","parent":"","content":""},{"id":8,"href":"/introduction/overview/","title":"Overview","parent":"Introduction","content":"    Glink Use Case Glink Architecture     Glink (Geographic Flink)是Flink在空间数据处理领域的扩展, 为Flink增加了兼容OGC标准的空间数据类型. Glink在DataStream API的基础之上构建了一层SpatialDataStream API, 增加了支持空间数据操作, 处理和分析的算子; 并在SQL API上扩展了符合SFA SQL规范的空间处理函数.\nGlink Use Case Glink主要用于带有空间属性的无界数据(无界空间数据)的流处理, 目前支持的功能如下下:\n Spatial Filter: 用于对无界空间数据进行空间过滤, 支持任意数量过类型的过滤几何, 同时支持任意类型的空间关系. Glink对常见的\u0026quot;选取一个或多个多边形内的点数据\u0026quot;这一场景进行了优化. Spatial KNN: 用于在无界空间数据的窗口快照上执行KNN. Spatial Join  Spatial Dimension Join: 用于无界空间数据与空间维度表的连接, 空间维度表支持以BroadcastStream的形式输入, 或存储在外部空间数据引擎(如GeoMesa)中. Spatial Window Join: 用于对两个无界空间数据集进行Window Join. Spatial Interval Join: 用于对两个无界空间数据集进行Interval Join.   Spatial Window DBSCAN: 用于在无界空间数据的窗口快照上执行DBSCAN聚类.  Glink Architecture Glink在Flink的基础之上增加了Spatial Data Stream Layer, Spatial Stream Processing Layer和Spatial API Layer. 其中, Spatial Data Stream Layer构建在Flink DataStream API的基础之上, 提供基于网格的空间分区, 并对外提供空间操作算子; Spatial Stream Processing Layer包含Glink的三种典型应用; Spatial API Layer提供两套使用Glink的 API, Spatial SQL API允许用户直接使用SQL和Glink提供的空间扩展函数进行相关操作, Java API允许用户编写Java代码调用SpatialDataStream API执行相应功能。\n"},{"id":9,"href":"/applications/","title":"Applications","parent":"","content":""},{"id":10,"href":"/introduction/getting-started/","title":"Getting Started","parent":"Introduction","content":"    Run Example of Glink Use Glink in Program     Run Example of Glink  在Release页面下载Glinker二进制发布包. 解压发布包.  tar -zxvf glink-x.x.x-bin.tar.gz 运行Glink提供的Spatial Filter示例.  nc -lk 9999 cd glink-x.x.x/examples flink run -c cn.edu.whu.glink.examples.datastream.SpatialFilterExample glink-examples-1.0.0.jar 该示例有两个查询几何, Polygon ((0 5,5 10,10 5,5 0,0 5))和Polygon ((5 5,10 10,15 5,10 0,5 5)), 用于过滤落在这两个多边形外部的点. 它监听9999端口, 可在nc -lk 9999窗口输入如下数据进行测试.\n1,114.35,34.50 2,2,2 Use Glink in Program  下载Glink源代码.  git clone git@github.com:glink-incubator/glink.git 编译并安装Glink依赖到本地仓库.  mvn clean install -DskipTests 在新工程的pom.xml中引入Glink依赖.  \u0026lt;properties\u0026gt; \u0026lt;glink.version\u0026gt;x.x.x\u0026lt;/glink.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;cn.edu.whu\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;glink-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${glink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;cn.edu.whu\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;glink-sql\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${glink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;cn.edu.whu\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;glink-connector-geomesa\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${glink.version}\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-log4j12\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; "},{"id":11,"href":"/glink_sql/","title":"Glink SQL","parent":"","content":""},{"id":12,"href":"/glink_sql/glink_sql_on_zepplin/","title":"Glink SQL on Zepplin","parent":"Glink SQL","content":"    Config Glink SQL on Zepplin     Glink SQL可以借助Flink的SQL API直接在Java代码中使用, 不过借助SQL客户端可以免去创建工程和配置依赖的繁琐. 目前开源可用的Flink SQL客户端主要有Flink自带的SQL Client的和Apache Zepplin. 我们将在Zepplin上使用Glink SQL.\n关于Zepplin的安装这里不再赘述, 如有问题可参考Flink on Zeppelin, 其中有详细的教程.\nConfig Glink SQL on Zepplin 要在Zepplin中使用Glink扩展的SQL函数需要在Flink Interpreter指定UDF Jar路径. 在flink.udf.jars配置项中增加$GLINK_HOME/lib/glink-sql-x.x.x.jar即可.\n"},{"id":13,"href":"/connectors/","title":"Connectors","parent":"","content":""},{"id":14,"href":"/posts/","title":"Blogs","parent":"","content":""},{"id":15,"href":"/dev_ctr/development/","title":"Development","parent":"Development \u0026 Contribution","content":"    Preparation Building Glink from Source Importing Flink into IntelliJ IDEA     Preparation 在开始开发之前, 先从代码仓库下载Glink源码.\ngit clone git@github.com:glink-incubator/glink.git Building Glink from Source 为从源码构建Glink, 需要安装Maven 3和Java 8. 安装完成后使用如下命令即可进行编译.\nmvn clean paclage -DskipTests Importing Flink into IntelliJ IDEA 我们使用IntelliJ IDEA开发Glink, 可通过如下步骤在IDEA中打开Glink项目:\n 打开IDEA, 选择\u0026quot;Open\u0026rdquo;; 选中Glink源代码所在文件夹, 点击\u0026quot;OK\u0026quot;即可打开; 等待IDEA自动导入项目.  Glink在编译时强制进行Checkstyle检查, 不通过则终止编译. 在IDEA中可使用\u0026quot;CheckStyle-IDEA\u0026quot;插件进行检查, 若未安装该插件, 通过如下步骤安装并配置Glink Checkstyle:\n 选择\u0026quot;File-\u0026gt;Settings-\u0026gt;Plugins\u0026rdquo;, 在搜索框输入\u0026quot;CheckStyle-IDEA\u0026quot;并搜索, 点击安装该插件; 重启IDEA; 选择\u0026quot;File-\u0026gt;Settings-\u0026gt;Tools-\u0026gt;Checkstyle\u0026rdquo;; 在\u0026quot;Scan Scope\u0026quot;中选择\u0026quot;Only java sources(including tests)\u0026quot;; 在\u0026quot;Configuration File\u0026quot;中点击\u0026quot;+\u0026rdquo;; 在弹出框中, \u0026ldquo;Description\u0026quot;填入\u0026quot;glink\u0026rdquo;, 点击\u0026quot;Browse\u0026rdquo;, 选择\u0026quot;glink/config/checkstyle.xml\u0026quot;文件. 点击\u0026quot;Next\u0026quot;完成即可.  完成上述步骤后, 即可在IDEA底部栏中看到\u0026quot;CheckStyle\u0026quot;选项卡, 点开即可进行扫描.\n"},{"id":16,"href":"/dev_ctr/","title":"Development \u0026 Contribution","parent":"","content":""},{"id":17,"href":"/dev_ctr/contribution/","title":"Contribution","parent":"Development \u0026 Contribution","content":"    Contribution to Glink Documentation Contribution to Glink Code     欢迎对Glink文档和代码进行任何形式的贡献.\nContribution to Glink Documentation Glink文档托管在https://github.com/glink-incubator/glink-docs仓库中, 采用Hugo进行渲染.\n对于简单的问题如拼写错误等直接提交commit即可, 对于新增页面或描述上的较大改动请创建一个Issue进行讨论, 注意Issue尽量使用英文. commit信息遵循如下规则:\n[hotfix] commit message [issue-number] commit message 在文档中注意如下规则:\n 标点符号一律使用英文标点符号; 中英文之间不加空格.  Contribution to Glink Code Glink源代码托管在https://github.com/glink-incubator/glink仓库中.\n对于简单的错误更正直接提交commit即可, 对于新增功能或较大改动请创建一个Issue进行讨论, 注意Issue尽量使用英文. commit信息遵循如下规则:\n[hotfix] commit message [issue-number] commit message "},{"id":18,"href":"/posts/initial-release/","title":"Glink SQL最佳实践 - GeoMesa结合应用","parent":"Blogs","content":"原文链接: Glink SQL最佳实践 - GeoMesa结合应用\n GeoMesa已经成为时空数据存储领域重要的索引中间件, 京东城市时空数据引擎JUST和阿里云的HBase Ganos均是在GeoMesa的基础上扩展而来. GeoMesa采用键值存储, 支持多种类型的存储后端, 如HBase, Kafka, Redis等. 相对于PostgreSQL+PostGIS这种基于R-tree索引的关系型存储, GeoMesa的存储方案更容易与HBase等现有的分布式数据库相结合, 从而直接利用底层数据库的分布式特性, 更适合时空大数据的存储以及实时场景的应用.\n为在时空流计算中利用GeoMesa的高效写入和时空查询能力, Glink扩展Flink SQL Connector框架形成了Flink GeoMesa SQL Connector(简称GeoMesa SQL Connector), 支持使用Flink SQL读写GeoMesa. 本文通过实际的应用案例, 讲述如何在Flink SQL中使用GeoMesa. 在流计算中Flink+GeoMesa主要有以下两种使用场景:\n 时空数据管道 \u0026amp; ETL: 以GeoMesa作为时空数据存储引擎, 通过Flink SQL构建实时的时空数据ETL管道, 将时空数据从文件, Kafka等数据源导入到GeoMesa; Spatial temporal join: 将维表存储在GeoMesa中, 通过Flink SQL进行流表与维表的空间join.  本文需要glink-0.1.2及以上版本, 可在zepplin中运行, 关于Glink及zepplin的安装配置参考Glink文档. 为方便复现笔者提供了可直接运行的glink-geomesa.zpln, 下载后可直接在zepplin打开运行.\n时空数据管道 \u0026amp; ETL 在IoT等行业, 产生的大量时空数据一般会接入到Kafka, 之后经过清洗, 转换, 增强存入时空数据库, 这就需要建设时空数据ETL管道. Flink在实时数据ETL管道建设中已经起到了重要作用. 然而Flink不支持空间数据类型, 同时也缺乏与空间数据库, 如GeoMesa等的Connector. 为此Glink增加了GeoMesa SQL Connector, 支持与GeoMesa进行交互, 方便了时空数据ETL管道的建设.\n在Glink中, 所有空间数据类型均用WKT格式的STRING类型表示, 同时通过Connector参数geomesa.spatial.fields指定空间类型字段和表示的几何类型. GeoMesa SQL Connector在写入GeoMesa时会将WKT转化为实际的几何对象. 下面通过一个实际的案例讲述如何利用Glink构建时空数据ETL管道.\n在该案例中我们将CSV文件中的数据通过Flink SQL导入到GeoMesa中. CSV文件中每行代表一个空间点, 总共包含四列, 每列的含义是: 点ID, 点生成时间, 经度, 纬度. 以下是一个简单的案例.\n1,2008-02-02 13:30:40,116.31412,70.89454 2,2008-02-02 13:30:44,116.31412,39.89454 3,2008-02-02 13:30:45,116.32674,39.89577 4,2008-02-02 13:30:49,116.31412,39.89454 这里将CSV文件作为source只是为了方便, source可以是Kafka, MySQL等Flink支持的任意组件.  首先创建source table, DDL语句如下.\nCREATE TABLE csv_table ( id STRING, dtg TIMESTAMP(0), lng DOUBLE, lat DOUBLE ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/path/to/file\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); 然后创建GeoMesa sink table.\nCREATE TABLE geomesa_table ( id STRING, dtg TIMESTAMP(0), point STRING, PRIMARY KEY (id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;geomesa\u0026#39;, \u0026#39;geomesa.data.store\u0026#39; = \u0026#39;hbase\u0026#39;, \u0026#39;geomesa.schema.name\u0026#39; = \u0026#39;geomesa_table\u0026#39;, \u0026#39;geomesa.spatial.fields\u0026#39; = \u0026#39;point:Point\u0026#39;, \u0026#39;hbase.catalog\u0026#39; = \u0026#39;geomesa_table\u0026#39; ); 最后通过如下语句即可实现将数据从CSV文件导入GeoMesa, 完成数据管道的构建.\nINSERT INTO geomesa_table SELECT id, dtg, ST_AsText(ST_Point(lng, lat)) FROM csv_table; Spatial Temporal Join 在流计算中, 流表与维表的关联是一项重要的基础功能. Flink可通过temporal join实现流表与维表的关联. 然而, 目前Flink的temporal join只支持等值join, 对于时空数据而言, 通常需要基于流表与维表中对象的空间关系进行join. 为此, Glink抽象出了spatial temporal join, 支持基于空间关系的temporal join, 目前Glink的spatial temporal join支持距离join, 相交join和包含join.\n{% colorquote info %} 的processing time temporal join. event time temporal join需要支持chengelog模式数据源, 无法在GeoMesa中实现. {% endcolorquote %}\nspatial temporal join具有大量的应用场景, 比如:\n 地理围栏应用, 流表中每条记录表示行人或车辆的轨迹点, 维表存储在GeoMesa中, 每条记录都是一个由多边形表示的地理围栏. 为了判断流表中的轨迹点是否出入了某个地理围栏, 可以将流表与维表做一个包含join, 若某个轨迹点被包含在某个多边形围栏中, 则这两条记录会执行join. 订单调度应用, 流表中每条记录都表示一个订单, 包含订单送达目的地的经纬度坐标, 维表存储在GeoMesa中, 每条记录都是由经纬度点表示的仓库位置. 为了获取与订单位置在某个距离范围内的仓库, 可以将流表与维表做一个距离join, 若订单位置与仓库位置在距离范围内, 则这两条记录会执行join.  在Glink中可以通过geomesa.temporal.join.predict这一Connector参数指定进行何种类型的空间join:\n R:\u0026lt;distance\u0026gt;表示距离join, 流表中空间对象与维表中空间对象距离在distance之内的记录都会被join, distance的单位为米; I表示相交join, 流表中空间对象与维表中空间对象在空间上相交的记录都会被join; +C表示正相交join, 流表中空间对象若在空间上包含维表中空间对象, 则两条记录会被join; -C表示负相交join, 维表中空间对象若在空间上包含流表中空间对象, 则两条记录会被join.  下面通过具体的案例讲述如何使用Glink进行spatial temporal join.\n相交/包含join 我们通过地理围栏应用讲述如何在Glink中进行相交join或包含join. 在地理围栏应用中, 流表中的一条记录通常是行人或车辆的轨迹点, 包含一些非空间属性及轨迹点的经纬度坐标. 维表中的一条记录通常代表一块地理区域, 包含一些非空间属性及地理区域的空间范围(由多边形表示). 在本例中, 流表来自CSV文件, 维表存储在GeoMesa中. 通过相交join或负相交join可以实现轨迹点与地理围栏的关联.\n首先定义流表, DDL语句如下.\nCREATE TABLE csv_point ( id STRING, dtg TIMESTAMP(0), lng DOUBLE, lat DOUBLE, proctime AS PROCTIME()) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/path/to/csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); GeoMesa维表的定义语句如下, geomesa.temporal.join.predict用于指定空间join的类型, 在地理围栏应用中使用I和-C可以达到相同的结果. 但是使用I有更高的效率.\nCREATE TABLE geomesa_area ( id STRING, dtg TIMESTAMP(0), geom STRING, PRIMARY KEY (id) NOT ENFORCED) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;geomesa\u0026#39;, \u0026#39;geomesa.data.store\u0026#39; = \u0026#39;hbase\u0026#39;, \u0026#39;geomesa.schema.name\u0026#39; = \u0026#39;restricted_area\u0026#39;, \u0026#39;geomesa.spatial.fields\u0026#39; = \u0026#39;geom:Polygon\u0026#39;, \u0026#39;geomesa.temporal.join.predict\u0026#39; = \u0026#39;I\u0026#39;, \u0026#39;hbase.zookeepers\u0026#39; = \u0026#39;localhost:2181\u0026#39;, \u0026#39;hbase.catalog\u0026#39; = \u0026#39;restricted_area\u0026#39; ); 通过如下语句进行spatial temporal join.\nSELECT A.id AS point_id, A.dtg, ST_AsText(ST_Point(A.lng, A.lat)) AS point, B.id AS area_id FROM csv_point AS A LEFT JOIN geomesa_area FOR SYSTEM_TIME AS OF A.proctime AS B ON ST_AsText(ST_Point(A.lng, A.lat)) = B.geom; 距离join 我们通过订单调度应用讲述如何在Glink中进行距离join. 在订单调度应用中, 流表中每条记录都表示一个订单, 包含相关的非空间属性及订单送达目的地的经纬度坐标; 维表存储在GeoMesa中, 每条记录都是由经纬度点表示的仓库位置. 在订单调度应用中, 通常需要为每个订单关联某个距离范围内的仓库, 用于订单的分发调度. 这可以通过Glink的距离join实现.\n首先定义流表, DDL语句如下.\nCREATE TABLE csv_order ( id STRING, dtg TIMESTAMP(0), lng DOUBLE, lat DOUBLE, proctime AS PROCTIME()) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;/path/to/csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); 然后定义GeoMesa维表.\nCREATE TABLE geomesa_warehouse ( id STRING, geom STRING, PRIMARY KEY (id) NOT ENFORCED) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;geomesa\u0026#39;, \u0026#39;geomesa.data.store\u0026#39; = \u0026#39;hbase\u0026#39;, \u0026#39;geomesa.schema.name\u0026#39; = \u0026#39;warehouse_point\u0026#39;, \u0026#39;geomesa.spatial.fields\u0026#39; = \u0026#39;geom:Point\u0026#39;, \u0026#39;geomesa.temporal.join.predict\u0026#39; = \u0026#39;R:400000\u0026#39;, \u0026#39;hbase.zookeepers\u0026#39; = \u0026#39;localhost:2181\u0026#39;, \u0026#39;hbase.catalog\u0026#39; = \u0026#39;warehouse_point\u0026#39; ); 最后通过如下语句即可实现距离join.\nSELECT A.id AS order_id, A.dtg, ST_AsText(ST_Point(A.lng, A.lat)) AS order_point, B.id AS warehouse_id FROM csv_order AS A LEFT JOIN geomesa_warehouse FOR SYSTEM_TIME AS OF A.proctime AS B ON ST_AsText(ST_Point(A.lng, A.lat)) = B.geom; 参考 [1] Flink SQL最佳实践 - HBase结合应用\n"},{"id":19,"href":"/tags/","title":"Tags","parent":"","content":""},{"id":20,"href":"/tags/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","title":"最佳实践","parent":"Tags","content":""},{"id":21,"href":"/_includes/","title":"Includes","parent":"","content":""},{"id":22,"href":"/_includes/include-page/","title":"Include Page","parent":"Includes","content":"Example page include\nExample Shortcode\nShortcode used in an include page.     Head 1 Head 2 Head 3     1 2 3    "},{"id":23,"href":"/","title":"","parent":"","content":"Glink - A Spatial Extension of Apache Flink [![Build Status](https://img.shields.io/drone/build/thegeeklab/hugo-geekdoc?logo=drone\u0026server=https%3A%2F%2Fdrone.thegeeklab.de)](https://drone.thegeeklab.de/thegeeklab/hugo-geekdoc) [![Hugo Version](https://img.shields.io/badge/hugo-0.83-blue.svg)](https://gohugo.io) --   \nGlink is an extension of Apache Flink in the field of spatial data. It adds spatial processing operators and supports spatial data types that conform to the OGC standard. Glink can make spatial stream data processing as simple as writing a \u0026ldquo;WordCount\u0026rdquo; program.\nGetting Started   Feature overview Spatial Filter Filter unbounded spatial data and optimize scenes with complex polygons as filter conditions.  Spatial Window KNN Perform a KNN query on the spatial data in each window snapshot.  Spatial Join Spatial join for unbounded spatial data, support stream and table join and double stream join.   Spatial Window DBSCAN Perform DBSCAN clustering on the spatial data in each window snapshot, and output the clustering results in real time.     "},{"id":24,"href":"/introduction/concepts/","title":"Concepts","parent":"Introduction","content":"    Unbounded Spatial Data (无界空间数据) Stream Geometry (流几何) Query Stream (查询几何) Spatial Dimension Table (空间维度表) Dimension Geometry (维度几何)     在Glink的源代码或本文档中, 为了实现更好的抽象, 我们引入了一些新的概念和名词, 这里进行集中介绍, 以方便读者更好地理解代码或文档中的内容.\nUnbounded Spatial Data (无界空间数据) 无界数据(Unbounded Data)的概念已经成熟, 无界空间数据是指带有空间属性的无界数据, 即数据中的每条记录都代表空间上的某个几何对象, 并可附带其他相关属性. 需要强调的一点是, 无界空间数据中的每条记录必须至少包含一个空间属性, 其他属性则是可选的. Glink中的SpatialDataStream便是一种无界空间数据的具体实现.\nStream Geometry (流几何) 无界空间数据中的单条记录称为流几何, 在Glink中用JTS的Geometry表示, 记录中的其他属性以Flink Tuple的形式存储在Geometry的userData成员变量中.\nQuery Stream (查询几何) 在无界空间数据的处理算子中, 输入的几何参数称为查询几何. 查询几何通常作为Spatial Filter或Spatial Join的条件.\nSpatial Dimension Table (空间维度表) 空间维度表表示带有空间属性的一系列数据集合, 其中的数据记录可以是不变的或缓慢变化的. 这里的空间维度表是一种抽象的概念, 因此其物理形态可以是多样的, 比如在Flink中以BroadcastDataStream的形态存在, 或者存储在外部的空间存储引擎(如GeoMesa)中.\nDimension Geometry (维度几何) 空间维度表中的单条记录称为维度几何.\n"}]